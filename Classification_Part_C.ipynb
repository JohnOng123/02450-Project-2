{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02450 Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal of Project 2 is to apply supervised learning techniques (regression and classification) to predict properties or classifications of wood based on the cleaned and scaled dataset from Project 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "from tqdm.notebook import tqdm\n",
    "import xlrd\n",
    "import scipy\n",
    "from scipy.linalg import svd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    plot,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from sklearn.naive_bayes import BernoulliNB  # Use BernoulliNB for binary classification\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import figure, hist, plot, show, subplot, xlabel, ylabel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from dtuimldmtools import *\n",
    "from dtuimldmtools.statistics.statistics import correlated_ttest\n",
    "from dtuimldmtools import confmatplot, rocplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned and Scaled Data from Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardwood= 1, Softwood = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_species_name                   0\n",
      "genus                                 0\n",
      "species                               0\n",
      "scientific_name                       0\n",
      "classification                        0\n",
      "moisture_content                      0\n",
      "specific_gravity                      0\n",
      "modulus_of_rupture                    0\n",
      "modulus_of_elasticity                 0\n",
      "work_to_maximum_load                  0\n",
      "compression_parallel_to_grain         0\n",
      "compression_perpendicular_to_grain    0\n",
      "shear_parallel_to_grain               0\n",
      "dtype: int64\n",
      "['Wood Type' 'Moisture Content' 'modulus_of_rupture'\n",
      " 'modulus_of_elasticity' 'work_to_maximum_load'\n",
      " 'compression_parallel_to_grain' 'compression_perpendicular_to_grain']\n",
      "1 (214, 7)\n",
      "2 (214, 9)\n",
      "X-shape (214, 8)\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "data = pd.read_csv(cwd + '/usa_wood_data_formatted.csv')\n",
    "\n",
    "removed_columns = ['side_hardness','tension_perpendicular_to_grain','impact_bending']\n",
    "data_cleaned = data.drop(columns=removed_columns)\n",
    "data_cleaned = data_cleaned.dropna()\n",
    "print(data_cleaned.isnull().sum()) # 0 means data is clean\n",
    "\n",
    "data_arr = np.array(data_cleaned)  # Attributes are columns\n",
    "attributeNames = [name for name in data_cleaned.columns]\n",
    "attributeNames = attributeNames[7:12]\n",
    "attributeNames = np.concatenate((['Wood Type', 'Moisture Content'], attributeNames))\n",
    "print(attributeNames)\n",
    "\n",
    "for i in range(data_arr.shape[1]):\n",
    "    val, count = np.unique(data_arr[:,i], return_counts=True)\n",
    "    #print(val.shape)        # Print how many unique values of each attribute exist \n",
    "                            # Probably dont do one of k coding for first 4 attributes since they have so many unique values\n",
    "\n",
    "data_adj = data_arr[:,6:13]\n",
    "print(\"1\",data_adj.shape)\n",
    "\n",
    "for i in range(2):\n",
    "    OoK = np.zeros([data_arr.shape[0], 1])\n",
    "    val, count = np.unique(data_arr[:,5 - i], return_counts=True)\n",
    "    \n",
    "    for j in range(data_arr.shape[0]):\n",
    "        if data_arr[j,5-i] == val[0]:\n",
    "            OoK[j, 0] = 1\n",
    "    data_adj = np.concatenate((OoK, data_adj), 1)\n",
    "print(\"2\",data_adj.shape)\n",
    "\n",
    "\n",
    "#classsification\n",
    "y = data_adj[:,0]\n",
    "X = data_adj[:,1:9]\n",
    "#X = np.concatenate(data_adj[:,2:9], 1)\n",
    "#regression\n",
    "#y = data_adj[:,4]\n",
    "# = np.concatenate((data_adj[:,0:3], data_adj[:,5:]), 1)\n",
    "N, M = X.shape\n",
    "\n",
    "X = X.astype(np.float32)  # Convert to float32 (or float64 if necessary)\n",
    "y = y.astype(np.float32)  # Convert y to float32\n",
    "\n",
    "#print(y)\n",
    "print(\"X-shape\", X.shape)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)  # Features as float32\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Target variable as float32 and reshaped to (N, 1)\n",
    "#X_tensor = torch.FloatTensor(X)\n",
    "#y_tensor = torch.FloatTensor(X_scaled[:,6])\n",
    "#y_tensor = torch.FloatTensor(y).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Two-Level Cross-Validation Model\n",
    "\n",
    "K1 = outer loop\n",
    "\n",
    "K2 = inner loop\n",
    "\n",
    "h = no. of hidden neurons\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks for Classification:\n",
    "\n",
    "Logistic regression\n",
    "\n",
    "Method 2\n",
    "2) ANN\n",
    "3) Classification tree\n",
    "4) k-nearest neighbor classification\n",
    "5) Na¨ıve Bayes\n",
    "\n",
    "Baseline:\n",
    "Compute largest class on the training data, and predict everything in the test-data as belonging to that class (corresponding to the optimal prediction by a logistic regression model with a bias\n",
    "term and no features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         logreg\u001b[38;5;241m.\u001b[39mfit(X_train,y_train)\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m logreg\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m---> 31\u001b[0m logistic_regression(X_train,y_train,X_test,y_test,\u001b[43mmode\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mode' is not defined"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train,y_train,X_test,y_test,mode):\n",
    "    if mode == \"return loss\":\n",
    "        X_train_mean = np.mean(X_train, axis=0)\n",
    "        X_train_std = np.std(X_train, axis=0)\n",
    "        # X_train_mean = np.expand_dims(X_train_mean, axis=1)\n",
    "        # X_train_std = np.expand_dims(X_train_std, axis=1)\n",
    "\n",
    "        for i in range(X_train.shape[0]):\n",
    "            X_train[i] = (X_train[i, :] - X_train_mean) / X_train_std\n",
    "        for i in range(X_test.shape[0]):\n",
    "            X_test[i] = (X_test[i, :] - X_train_mean) / X_train_std\n",
    "\n",
    "        logreg = lm.LogisticRegression(solver='lbfgs', multi_class='multinomial', tol=1e-4, random_state=1, max_iter=10000,penalty='l2', C=1/l)\n",
    "        logreg.fit(X_train,y_train)\n",
    "        return np.sum(logreg.predict(X_test)!=y_test)/len(y_test)\n",
    "    elif mode == \"return predictions\":\n",
    "        X_train_mean = np.mean(X_train, axis=0)\n",
    "        X_train_std = np.std(X_train, axis=0)\n",
    "        # X_train_mean = np.expand_dims(X_train_mean, axis=1)\n",
    "        # X_train_std = np.expand_dims(X_train_std, axis=1)\n",
    "\n",
    "        for i in range(X_train.shape[0]):\n",
    "            X_train[i] = (X_train[i, :] - X_train_mean) / X_train_std\n",
    "        for i in range(X_test.shape[0]):\n",
    "            X_test[i] = (X_test[i, :] - X_train_mean) / X_train_std\n",
    "\n",
    "        logreg = lm.LogisticRegression(solver='lbfgs', multi_class='multinomial', tol=1e-4, random_state=1, max_iter=10000,penalty='l2', C=1/l)\n",
    "        logreg.fit(X_train,y_train)\n",
    "        return logreg.predict(X_test)\n",
    "        \n",
    "logistic_regression(X_train,y_train,X_test,y_test,mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu (mean of X_train): [4.9740931e-01 4.9243513e-01 9.7362695e+03 1.3914505e+00 1.1073053e+01\n",
      " 4.8658550e+03 6.6647668e+02 1.2485492e+03]\n",
      "sigma (std of X_train): [4.9999356e-01 1.1432965e-01 3.5229573e+03 3.2024434e-01 5.1639738e+00\n",
      " 1.8886897e+03 4.0491800e+02 4.3628192e+02]\n",
      "\n",
      "Lambda = 0.01 (C = 100.0):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.010985411419875584 (C = 91.02981779915217):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.012067926406393288 (C = 82.86427728546843):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.013257113655901088 (C = 75.4312006335462):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.014563484775012436 (C = 68.66488450043002):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.015998587196060583 (C = 62.505519252739724):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.017575106248547922 (C = 56.89866029018296):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.019306977288832496 (C = 51.794746792312125):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.021209508879201904 (C = 47.14866363457394):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.023299518105153717 (C = 42.91934260128779):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.025595479226995357 (C = 39.06939937054617):\n",
      "Train Error Rate: 0.13471502590673576\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.028117686979742307 (C = 35.564803062231285):\n",
      "Train Error Rate: 0.13471502590673576\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.030888435964774818 (C = 32.374575428176435):\n",
      "Train Error Rate: 0.13471502590673576\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.03393221771895328 (C = 29.470517025518113):\n",
      "Train Error Rate: 0.13471502590673576\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.0372759372031494 (C = 26.826957952797258):\n",
      "Train Error Rate: 0.13471502590673576\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.040949150623804234 (C = 24.420530945486522):\n",
      "Train Error Rate: 0.13471502590673576\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.04498432668969444 (C = 22.229964825261955):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.04941713361323833 (C = 20.235896477251575):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.054286754393238594 (C = 18.420699693267164):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.05963623316594643 (C = 16.768329368110084):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.0655128556859551 (C = 15.264179671752334):\n",
      "Train Error Rate: 0.13989637305699482\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.07196856730011521 (C = 13.894954943731376):\n",
      "Train Error Rate: 0.13989637305699482\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.07906043210907697 (C = 12.648552168552966):\n",
      "Train Error Rate: 0.13989637305699482\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.08685113737513525 (C = 11.513953993264474):\n",
      "Train Error Rate: 0.13989637305699482\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.09540954763499938 (C = 10.481131341546858):\n",
      "Train Error Rate: 0.13989637305699482\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.10481131341546858 (C = 9.540954763499938):\n",
      "Train Error Rate: 0.13989637305699482\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.1151395399326447 (C = 8.685113737513529):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.12648552168552957 (C = 7.906043210907702):\n",
      "Train Error Rate: 0.12435233160621761\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.13894954943731375 (C = 7.196856730011521):\n",
      "Train Error Rate: 0.12435233160621761\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.15264179671752334 (C = 6.551285568595509):\n",
      "Train Error Rate: 0.12435233160621761\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.16768329368110074 (C = 5.963623316594646):\n",
      "Train Error Rate: 0.12435233160621761\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.18420699693267154 (C = 5.428675439323862):\n",
      "Train Error Rate: 0.12435233160621761\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.20235896477251566 (C = 4.941713361323836):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.22229964825261944 (C = 4.498432668969446):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.2442053094548651 (C = 4.094915062380426):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.2682695795279725 (C = 3.7275937203149416):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.29470517025518095 (C = 3.39322177189533):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.32374575428176433 (C = 3.088843596477482):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.35564803062231287 (C = 2.8117686979742307):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.3906939937054615 (C = 2.559547922699537):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.42919342601287763 (C = 2.329951810515373):\n",
      "Train Error Rate: 0.11398963730569948\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.47148663634573923 (C = 2.1209508879201913):\n",
      "Train Error Rate: 0.11398963730569948\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.517947467923121 (C = 1.9306977288832508):\n",
      "Train Error Rate: 0.11398963730569948\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.5689866029018296 (C = 1.7575106248547923):\n",
      "Train Error Rate: 0.11398963730569948\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.6250551925273969 (C = 1.599858719606059):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 0.6866488450042998 (C = 1.4563484775012445):\n",
      "Train Error Rate: 0.11917098445595854\n",
      "Test Error Rate: 0.23809523809523808\n",
      "\n",
      "Lambda = 0.7543120063354615 (C = 1.3257113655901096):\n",
      "Train Error Rate: 0.12435233160621761\n",
      "Test Error Rate: 0.23809523809523808\n",
      "\n",
      "Lambda = 0.8286427728546842 (C = 1.206792640639329):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.23809523809523808\n",
      "\n",
      "Lambda = 0.9102981779915218 (C = 1.0985411419875584):\n",
      "Train Error Rate: 0.12953367875647667\n",
      "Test Error Rate: 0.19047619047619047\n",
      "\n",
      "Lambda = 1.0 (C = 1.0):\n",
      "Train Error Rate: 0.13471502590673576\n",
      "Test Error Rate: 0.19047619047619047\n",
      "Optimal Lambda: 0.01\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression model: Evaluating classification \n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "CV = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "#def logistic_regression(X_train,y_train,X_test,y_test,l,mode):\n",
    "\n",
    "# Standardize the training and test set based on training set mean and std\n",
    "mu = np.mean(X_train, 0)\n",
    "sigma = np.std(X_train, 0)\n",
    "\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma\n",
    "print(\"mu (mean of X_train):\", mu)\n",
    "print(\"sigma (std of X_train):\", sigma)\n",
    "\n",
    "# Fit regularized logistic regression model to training data to predict the type of wine\n",
    "#lambda_interval = np.logspace(-8, 2, 50)\n",
    "lambda_interval = np.logspace(-2, 0, 50)  # From 10^-2 to 10^0 (0.01 to 1) with 50 points\n",
    "#lambda_interval = np.power(10.0, range(-5, 9)) # Values of lambda from Part. XX write about this\n",
    "# Check lambda values and intervals\n",
    "#print(\"Lambda interval:\", lambda_interval)\n",
    "\n",
    "train_error_rate = np.zeros(len(lambda_interval))\n",
    "test_error_rate = np.zeros(len(lambda_interval))\n",
    "coefficient_norm = np.zeros(len(lambda_interval))\n",
    "\n",
    "for k in range(0, len(lambda_interval)):\n",
    "    mdl = LogisticRegression(penalty=\"l2\", C=1 / lambda_interval[k]) #what exactly does Max Iter do?!\n",
    "\n",
    "    mdl.fit(X_train, y_train)\n",
    "\n",
    "    y_train_est = mdl.predict(X_train).T\n",
    "    y_test_est = mdl.predict(X_test).T\n",
    "\n",
    "    train_error_rate[k] = np.sum(y_train_est != y_train) / len(y_train)\n",
    "    test_error_rate[k] = np.sum(y_test_est != y_test) / len(y_test)\n",
    "\n",
    "    w_est = mdl.coef_[0]\n",
    "    coefficient_norm[k] = np.sqrt(np.sum(w_est**2))\n",
    "    print(f\"\\nLambda = {lambda_interval[k]} (C = {1 / lambda_interval[k]}):\")\n",
    "    print(\"Train Error Rate:\", train_error_rate[k])\n",
    "    print(\"Test Error Rate:\", test_error_rate[k])\n",
    "    #print(\"Coefficient Norm:\", coefficient_norm[k])\n",
    "\n",
    "min_error = np.min(test_error_rate)\n",
    "opt_lambda_idx = np.argmin(test_error_rate)\n",
    "opt_lambda = lambda_interval[opt_lambda_idx]\n",
    "#print(w_est)\n",
    "#print(y_train)\n",
    "#print(y_train_est)\n",
    "#print(y_test_est)\n",
    "print(\"Optimal Lambda:\",opt_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mylim([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m25\u001b[39m])\n\u001b[1;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogx(lambda_interval, coefficient_norm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/pyplot.py:612\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/IPython/core/formatters.py:182\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    180\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/IPython/core/formatters.py:226\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/IPython/core/formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    345\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/IPython/core/pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 170\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/backend_bases.py:2175\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2172\u001b[0m     \u001b[38;5;66;03m# we do this instead of `self.figure.draw_without_rendering`\u001b[39;00m\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;66;03m# so that we can inject the orientation\u001b[39;00m\n\u001b[1;32m   2174\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2175\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/figure.py:3162\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3162\u001b[0m     \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3165\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/axes/_base.py:3137\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3135\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3137\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3140\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/axis.py:1423\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m renderer\u001b[38;5;241m.\u001b[39mopen_group(\u001b[38;5;18m__name__\u001b[39m, gid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gid())\n\u001b[0;32m-> 1423\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m tlb1, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks_to_draw:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/axis.py:1309\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1307\u001b[0m minor_locs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_minorticklocs()\n\u001b[1;32m   1308\u001b[0m minor_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminor\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mformat_ticks(minor_locs)\n\u001b[0;32m-> 1309\u001b[0m minor_ticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_minor_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mminor_locs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick, loc, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(minor_ticks, minor_locs, minor_labels):\n\u001b[1;32m   1311\u001b[0m     tick\u001b[38;5;241m.\u001b[39mupdate_position(loc)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/axis.py:1696\u001b[0m, in \u001b[0;36mAxis.get_minor_ticks\u001b[0;34m(self, numticks)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     numticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_minorticklocs())\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminorTicks) \u001b[38;5;241m<\u001b[39m numticks:\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# Update the new tick label properties from the old.\u001b[39;00m\n\u001b[0;32m-> 1696\u001b[0m     tick \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminorTicks\u001b[38;5;241m.\u001b[39mappend(tick)\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_tick_props(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminorTicks[\u001b[38;5;241m0\u001b[39m], tick)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/axis.py:1598\u001b[0m, in \u001b[0;36mAxis._get_tick\u001b[0;34m(self, major)\u001b[0m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Axis subclass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must define \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1596\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tick_class or reimplement _get_tick()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1597\u001b[0m tick_kw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major_tick_kw \u001b[38;5;28;01mif\u001b[39;00m major \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_minor_tick_kw\n\u001b[0;32m-> 1598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tick_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmajor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtick_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/axis.py:406\u001b[0m, in \u001b[0;36mXTick.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# the y loc is 3 points below the min of y axis\u001b[39;00m\n\u001b[1;32m    405\u001b[0m trans, va, ha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_text1_transform()\n\u001b[0;32m--> 406\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverticalalignment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mva\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizontalalignment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m trans, va, ha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_text2_transform()\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mset(\n\u001b[1;32m    412\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    413\u001b[0m     verticalalignment\u001b[38;5;241m=\u001b[39mva, horizontalalignment\u001b[38;5;241m=\u001b[39mha, transform\u001b[38;5;241m=\u001b[39mtrans,\n\u001b[1;32m    414\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/artist.py:147\u001b[0m, in \u001b[0;36mArtist.__init_subclass__.<locals>.<lambda>\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_autogenerated_signature\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Don't overwrite cls.set if the subclass or one of its parents\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# has defined a set method set itself.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# If there was no explicit definition, cls.set is inherited from\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# the hierarchy of auto-generated set methods, which hold the\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# flag _autogenerated_signature.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mArtist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/artist.py:1224\u001b[0m, in \u001b[0;36mArtist.set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;66;03m# docstring and signature are auto-generated via\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;66;03m# Artist._update_set_signature_and_docstring() at the end of the\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;66;03m# module.\u001b[39;00m\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_update(\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/cbook.py:1834\u001b[0m, in \u001b[0;36mnormalize_kwargs\u001b[0;34m(kw, alias_mapping)\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(alias_mapping, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(alias_mapping, Artist)\n\u001b[1;32m   1831\u001b[0m       \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(alias_mapping, Artist)):\n\u001b[1;32m   1832\u001b[0m     alias_mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(alias_mapping, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_alias_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m-> 1834\u001b[0m to_canonical \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43malias\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanonical\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43malias_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43malias_list\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m   1837\u001b[0m canonical_to_seen \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1838\u001b[0m ret \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# output dictionary\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MachLearn/lib/python3.11/site-packages/matplotlib/cbook.py:1834\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(alias_mapping, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(alias_mapping, Artist)\n\u001b[1;32m   1831\u001b[0m       \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(alias_mapping, Artist)):\n\u001b[1;32m   1832\u001b[0m     alias_mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(alias_mapping, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_alias_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m-> 1834\u001b[0m to_canonical \u001b[38;5;241m=\u001b[39m {alias: canonical\n\u001b[1;32m   1835\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m canonical, alias_list \u001b[38;5;129;01min\u001b[39;00m alias_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1836\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m alias \u001b[38;5;129;01min\u001b[39;00m alias_list}\n\u001b[1;32m   1837\u001b[0m canonical_to_seen \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1838\u001b[0m ret \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# output dictionary\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#plots for this?\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "#plt.plot(np.log10(lambda_interval), train_error_rate*100)\n",
    "#plt.plot(np.log10(lambda_interval), test_error_rate*100)\n",
    "#plt.plot(np.log10(opt_lambda), min_error*100, 'o')\n",
    "\n",
    "plt.semilogx(lambda_interval, train_error_rate * 100)\n",
    "plt.semilogx(lambda_interval, test_error_rate * 100)\n",
    "plt.semilogx(opt_lambda, min_error * 100, \"o\")\n",
    "plt.text(\n",
    "    1e-2,\n",
    "    0,\n",
    "    \"Minimum test error: \"\n",
    "    + str(np.round(min_error * 100, 2))\n",
    "    + \" % at 1e\"\n",
    "    + str(np.round(np.log10(opt_lambda), 2)),\n",
    ")\n",
    "plt.xlabel(\"Regularization strength, $\\log_{10}(\\lambda)$\")\n",
    "plt.ylabel(\"Error rate (%)\")\n",
    "plt.title(\"Classification error\")\n",
    "plt.legend([\"Training error\", \"Test error\", \"Test minimum\"], loc=\"upper right\")\n",
    "plt.ylim([0, 25])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.semilogx(lambda_interval, coefficient_norm, \"k\")\n",
    "plt.ylabel(\"L2 Norm\")\n",
    "plt.xlabel(\"Regularization strength, $\\log_{10}(\\lambda)$\")\n",
    "plt.title(\"Parameter vector L2 norm\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation fold: 1/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Cross-validation fold: 2/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Cross-validation fold: 3/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Cross-validation fold: 4/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Cross-validation fold: 5/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Cross-validation fold: 6/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Cross-validation fold: 7/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Cross-validation fold: 8/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Cross-validation fold: 9/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Cross-validation fold: 10/10\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean Test Error Rate: 41.15%\n",
      "Average Train Error Rate: 41.12%\n",
      "Average Test Error Rate: 41.15%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def naive_classifier_binary(X, y):\n",
    "    np.random.seed(2450)\n",
    "    y = y.squeeze()\n",
    "    \n",
    "# Example usage with lambda and alpha ranges\n",
    "#lambda_values = np.logspace(-5, 1, 10)  # Replace with desired range for logistic regression\n",
    "#alpha_values = np.logspace(-5, 1, 10)   # Replace with desired range for naive classifier\n",
    "\n",
    "\n",
    "    # Naive Bayes classifier parameters for binary features\n",
    "    alpha = 1.0  # Laplace smoothing\n",
    "    fit_prior = True  # Change if you want uniform prior\n",
    "    \n",
    "    # Set up K-fold cross-validation\n",
    "    K = 10\n",
    "    #CV = KFold(n_splits=K, shuffle=True)\n",
    "    CV = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "    errors = np.zeros(K)\n",
    "    train_errors = np.zeros(K)\n",
    "    test_errors = np.zeros(K)\n",
    "    k = 0\n",
    "\n",
    "    \n",
    "    for train_index, test_index in CV.split(X):\n",
    "        print(f'Cross-validation fold: {k+1}/{K}')\n",
    "        \n",
    "        # Split data into training and test sets\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Initialize and fit BernoulliNB for binary classification\n",
    "        nb_classifier = BernoulliNB(alpha=alpha, fit_prior=fit_prior)\n",
    "        nb_classifier.fit(X_train, y_train)\n",
    "        \n",
    "         # Predictions for test and train sets\n",
    "        y_test_est = nb_classifier.predict(X_test)\n",
    "        y_train_est = nb_classifier.predict(X_train)\n",
    "\n",
    "        # Calculate error rates\n",
    "        test_errors[k] = np.mean(y_test_est != y_test)\n",
    "        train_errors[k] = np.mean(y_train_est != y_train)\n",
    "        \n",
    "        # Predict and calculate error\n",
    "        y_pred = nb_classifier.predict(X_test)\n",
    "        errors[k] = np.mean(y_pred != y_test)\n",
    "        print(y_pred)\n",
    "        k += 1\n",
    "\n",
    "    # Print mean error across folds\n",
    "    print(f\"Mean Test Error Rate: {100 * np.mean(errors):.2f}%\")\n",
    "     # Plot the classification error rate\n",
    "    #print(\"Error rate: {0}%\".format(100 * np.mean(errors))) \n",
    "    #print(\"Coefficient Norm:\", coefficient_norm[k])\n",
    "    print(\"Average Train Error Rate: {:.2f}%\".format(100 * np.mean(train_errors)))\n",
    "    print(\"Average Test Error Rate: {:.2f}%\".format(100 * np.mean(test_errors)))\n",
    "\n",
    "naive_classifier_binary(X, y)  # Call the function with appropriate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent class is: 1.0\n",
      "Average Test Error Rate: 0.43\n",
      "Baseline Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def baseline_classification(y_train,y_test):  \n",
    "    error_rates = []\n",
    "    # Baseline\n",
    "    K = 10\n",
    "    CV = model_selection.KFold(n_splits=K, shuffle=True)\n",
    "\n",
    "    # Find the most frequent class in the training labels\n",
    "    unique_classes, counts = np.unique(y_train, return_counts=True)\n",
    "    most_frequent_class = unique_classes[np.argmax(counts)]\n",
    "    print(\"The most frequent class is:\", most_frequent_class)\n",
    "\n",
    "    #Create predictions for the test set using the most frequent class\n",
    "    y_pred_baseline = np.full(y_test.shape, most_frequent_class)  # predict the most frequent class for all\n",
    "\n",
    "    #Evaluate the baseline model\n",
    "    baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "    baseline_error = 1 - baseline_accuracy\n",
    "    error_rates.append(baseline_error)\n",
    "\n",
    "    # Calculate the average test error rate across all folds\n",
    "    avg_test_error_rate = np.mean(error_rates)\n",
    "    print(f\"Average Test Error Rate: {avg_test_error_rate:.2f}\")\n",
    "    print(f\"Baseline Accuracy: {baseline_accuracy:.2f}\")\n",
    "baseline_classification(y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m alpha_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)   \u001b[38;5;66;03m# Replace with desired range for naive classifier\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Run cross-validation\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m results \u001b[38;5;241m=\u001b[39m cross_validation(\u001b[43mX_train_tensor\u001b[49m, y_train_tensor, lambda_values\u001b[38;5;241m=\u001b[39mlambda_values, alpha_values\u001b[38;5;241m=\u001b[39malpha_values)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "#2 level crossvalidation\n",
    "\n",
    "def cross_validation(X, y, K1=10, K2=10, lambda_values=None, alpha_values=None):\n",
    "    # Define outer and inner cross-validation objects\n",
    "    outer_cv = KFold(n_splits=K1, shuffle=True, random_state=42)\n",
    "    inner_cv = KFold(n_splits=K2, shuffle=True, random_state=42)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Outer cross-validation loop\n",
    "    for outer_fold, (train_idx, test_idx) in enumerate(outer_cv.split(X), start=1):\n",
    "        X_train_outer, X_test_outer = X[train_idx], X[test_idx]\n",
    "        y_train_outer, y_test_outer = y[train_idx], y[test_idx]\n",
    "\n",
    "        best_lambda = None\n",
    "        best_alpha = None\n",
    "        lowest_error_logistic = float('inf')\n",
    "        lowest_error_naive = float('inf')\n",
    "\n",
    "        # Inner cross-validation loop for hyperparameter tuning\n",
    "        for train_idx_inner, val_idx_inner in inner_cv.split(X_train_outer):\n",
    "            X_train_inner, X_val_inner = X[train_idx_inner], X[train_idx_inner]\n",
    "            y_train_inner, y_val_inner = y[train_idx_inner], y[val_idx_inner]\n",
    "\n",
    "            # Logistic Regression - finding best lambda\n",
    "            for lambda_ in lambda_values:\n",
    "                error_logistic = logistic_regression(X_train_inner, y_train_inner, X_val_inner, y_val_inner, lambda_)\n",
    "                if error_logistic < lowest_error_logistic:\n",
    "                    lowest_error_logistic = error_logistic\n",
    "                    best_lambda = lambda_\n",
    "\n",
    "            # Naive Bayes - finding best alpha\n",
    "            for alpha in alpha_values:\n",
    "                error_naive = naive_classifier_binary(X_train_inner, y_train_inner, X_val_inner, y_val_inner, alpha)\n",
    "                if error_naive < lowest_error_naive:\n",
    "                    lowest_error_naive = error_naive\n",
    "                    best_alpha = alpha\n",
    "\n",
    "        # Evaluate on the outer test set with the best hyperparameters\n",
    "        test_error_logistic = logistic_regression(X_train_outer, y_train_outer, X_test_outer, y_test_outer, best_lambda)\n",
    "        test_error_naive = naive_classifier_binary(X_train_outer, y_train_outer, X_test_outer, y_test_outer, best_alpha)\n",
    "        baseline_error = baseline_classification(y_train_outer, y_test_outer)\n",
    "\n",
    "        results.append((outer_fold, best_lambda, test_error_logistic, best_alpha, test_error_naive, baseline_error))\n",
    "\n",
    "    # Display results\n",
    "    print(f\"{'Fold':<6} {'Best Lambda':<12} {'Logistic Error':<15} {'Best Alpha':<10} {'Naive Error':<12} {'Baseline Error'}\")\n",
    "    for result in results:\n",
    "        print(f\"{result[0]:<6} {result[1]:<12} {result[2]:<15.4f} {result[3]:<10} {result[4]:<12.4f} {result[5]:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage with lambda and alpha ranges\n",
    "lambda_values = np.logspace(-5, 1, 10)  # Replace with desired range for logistic regression\n",
    "alpha_values = np.logspace(-5, 1, 10)   # Replace with desired range for naive classifier\n",
    "\n",
    "# Run cross-validation\n",
    "results = cross_validation(X_train_tensor, y_train_tensor, lambda_values=lambda_values, alpha_values=alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_out_of_k_to_numbers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m old_X \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m     10\u001b[0m one_out_of_y \u001b[38;5;241m=\u001b[39m X[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:]\n\u001b[0;32m---> 11\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mone_out_of_k_to_numbers\u001b[49m(one_out_of_y)\n\u001b[1;32m     12\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[old_X[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m],old_y]\n\u001b[1;32m     14\u001b[0m K1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'one_out_of_k_to_numbers' is not defined"
     ]
    }
   ],
   "source": [
    "# 2 Level crossvalidation\n",
    "import numpy as np\n",
    "#from data_import import *\n",
    "from sklearn import model_selection\n",
    "#from dataProcessing import *\n",
    "#from utils import *\n",
    "\n",
    "old_y = np.where( y != 0, np.ma.log10(y),0)\n",
    "old_X = X\n",
    "one_out_of_y = X[:,-4:]\n",
    "y = one_out_of_k_to_numbers(one_out_of_y)\n",
    "X = np.c_[old_X[:,:-4],old_y]\n",
    "\n",
    "K1 = 10\n",
    "K2 = 10\n",
    "\n",
    "#model parameters\n",
    "lambdas = np.logspace(-4, .5, num=100, endpoint=True, base=10.0)\n",
    "hs = []#[1,2,3,5,10,20,30,50,100]\n",
    "alphas = [0.1, 1.0, 10.0]  # Example values for alpha, you can adjust these\n",
    "models = \"logistic regression lambdas: \" + np.array2string(np.array(lambdas)) + \"; ANN hs: \"+ np.array2string(np.array(hs))\n",
    "with open(\"outputs/class/classification_models.txt\",\"w\") as file:\n",
    "    file.write(models)\n",
    "    file.close()\n",
    "number_of_logreg_models = len(lambdas)\n",
    "number_of_ANNs = len(hs)\n",
    "#inner and outer cross-validation splits\n",
    "outer_CV = model_selection.KFold(K1,shuffle=True)\n",
    "inner_CV = model_selection.KFold(K2,shuffle=True)\n",
    "\n",
    "\n",
    "outer_fold = 0\n",
    "outer_error_array = np.zeros(K1)\n",
    "selected_models = np.zeros(K1)\n",
    "for outer_train_index, outer_test_index in outer_CV.split(X,y):\n",
    "    outer_X_train = X[outer_train_index].astype('float64')\n",
    "    outer_y_train = y[outer_train_index].astype('float64')\n",
    "    outer_X_test = X[outer_test_index].astype('float64')\n",
    "    outer_y_test = y[outer_test_index].astype('float64')\n",
    "    #Inner cross-validation loop\n",
    "    inner_error_matrix = np.zeros((K2,number_of_logreg_models+number_of_ANNs+1))\n",
    "    inner_genE_array = np.zeros(number_of_logreg_models+number_of_ANNs+1)\n",
    "    inner_fold = 0\n",
    "    for inner_train_index, inner_test_index in inner_CV.split(outer_X_train,outer_y_train):\n",
    "        inner_X_train = outer_X_train[inner_train_index]\n",
    "        inner_y_train = outer_y_train[inner_train_index]\n",
    "        inner_X_test = outer_X_train[inner_test_index]\n",
    "        inner_y_test = outer_y_train[inner_test_index]\n",
    "        \n",
    "        for l in range(len(lambdas)):\n",
    "            inner_error_matrix[inner_fold,l] = logistic_regression(inner_X_train,inner_y_train,inner_X_test,inner_y_test,lambdas[l])\n",
    "            print(\"LogReg Nr. \"+str(l)+\" error: \"+str( inner_error_matrix[inner_fold,l]))\n",
    "        \n",
    "        for h in range(len(hs)):\n",
    "            inner_error_matrix[inner_fold,number_of_logreg_models+h] = ANN_gen_error(inner_X_train,inner_y_train,inner_X_test,inner_y_test,hs[h],\"classification\")\n",
    "            print(\"ANN Nr. \"+str(l)+\" error: \"+str( inner_error_matrix[inner_fold,number_of_logreg_models+h]))\n",
    "        \n",
    "        inner_error_matrix[inner_fold,-1] = baseline_classification_model(inner_y_train,inner_y_test)\n",
    "        print(\"Baseline: \"+str(inner_error_matrix[inner_fold,-1]))\n",
    "        \n",
    "        # Naive Bayes classifier errors with different alpha values\n",
    "        for a, alpha in enumerate(alphas):\n",
    "            inner_error_matrix[inner_fold, number_of_logreg_models + number_of_ann_models + a] = naive_classifier(inner_X_train, inner_y_train, inner_X_test, inner_y_test, alpha)\n",
    "            print(\"Naive Bayes alpha=\" + str(alpha) + \" error: \" + str(inner_error_matrix[inner_fold, number_of_logreg_models + number_of_ann_models + a]))\n",
    "\n",
    "        \n",
    "        print(\"inner fold \"+str(inner_fold)+\" done\")\n",
    "        inner_fold = inner_fold + 1\n",
    "    \n",
    "    np.savetxt(\"outputs/class/classification_OuterFold_\"+str(outer_fold)+\"_inner_error_matrix.csv\",inner_error_matrix,delimiter=\",\",fmt='%1.10f')\n",
    "    for index in range(len(inner_genE_array)):\n",
    "        inner_genE_array[index] = inner_error_matrix.T[index].mean()\n",
    "    model = np.where( inner_genE_array == inner_genE_array.min())[0][0]\n",
    "    selected_models[outer_fold] = model # save the selected model\n",
    "    if model in range(number_of_logreg_models):\n",
    "        outer_error_array[outer_fold] = logistic_regression(outer_X_train,outer_y_train,outer_X_test,outer_y_test,lambdas[model])\n",
    "    elif model in range(number_of_logreg_models,number_of_logreg_models+number_of_ANNs):\n",
    "        outer_error_array[outer_fold] = ANN_gen_error(outer_X_train,outer_y_train,outer_X_test,outer_y_test,hs[model-number_of_logreg_models],\"classification\")\n",
    "    else:\n",
    "        outer_error_array[outer_fold] =baseline_classification_model(outer_y_train,outer_y_test)\n",
    "    print(\"Gen error: \"+str(outer_error_array[outer_fold]))\n",
    "    print(\"outer Fold \"+str(outer_fold) +\" done\")\n",
    "    outer_fold = outer_fold + 1\n",
    "np.savetxt(\"outputs/class/classification_outer_error_array.csv\",outer_error_array,delimiter=\",\",fmt='%1.10f')\n",
    "np.savetxt(\"outputs/class/classification_selected_models.csv\",selected_models,delimiter=\",\",fmt='%1.10f')\n",
    "print(outer_error_array)\n",
    "print(\"Gen Error: \" + str(outer_error_array.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold | h* | Test Error\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold | h* | Test Error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_results\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m h \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m test_results]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_results' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Fold | h* | Test Error\")\n",
    "for r in test_results:\n",
    "    print(f\"{r[0]} | {r[1]} | {r[2]}\")\n",
    "\n",
    "h = [r[1] for r in test_results]\n",
    "\n",
    "# Plot the histogram for the frequency plot of h\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(h, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xticks(range(int(min(h))-1, int(max(h))+2, 1))\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Plot for Number h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Compute the Jeffreys interval\u001b[39;00m\n\u001b[1;32m      6\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[0;32m----> 7\u001b[0m [thetahat, CI, p] \u001b[38;5;241m=\u001b[39m mcnemar(\u001b[43my_true\u001b[49m, yhat[:, \u001b[38;5;241m0\u001b[39m], yhat[:, \u001b[38;5;241m1\u001b[39m], alpha\u001b[38;5;241m=\u001b[39malpha)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta = theta_A-theta_B point estimate\u001b[39m\u001b[38;5;124m\"\u001b[39m, thetahat, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m CI: \u001b[39m\u001b[38;5;124m\"\u001b[39m, CI, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp-value\u001b[39m\u001b[38;5;124m\"\u001b[39m, p)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "#statistical evaluation of your three models, McNemera’s test described in Box 11.3.2\n",
    "\n",
    "from dtuimldmtools import mcnemar\n",
    "\n",
    "# Compute the Jeffreys interval\n",
    "alpha = 0.05\n",
    "[thetahat, CI, p] = mcnemar(y_true, yhat[:, 0], yhat[:, 1], alpha=alpha)\n",
    "\n",
    "print(\"theta = theta_A-theta_B point estimate\", thetahat, \" CI: \", CI, \"p-value\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Use the features identified as significant from the PCA analysis. \n",
    "\n",
    "This includes 'modulus of rupture', 'shear parallel to grain', 'compression parallel to grain', 'modulus of elasticity', 'work to maximum load', and 'specific gravity'. \n",
    "\n",
    "These are strong candidates for your predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old stuff/ Inspiration\n",
    "\n",
    "To predict a continuous variable based on other variables. \n",
    "\n",
    "To predict the values/property of one feature based on other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Model\n",
    "\n",
    "# Define variables\n",
    "K1 = 10\n",
    "K2 = 10\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim, h):\n",
    "        super(ANN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, h)\n",
    "        self.layer2 = nn.Linear(h,h)\n",
    "        self.output_layer = nn.Linear(h, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Cross-validation\n",
    "outer_cv = KFold(n_splits=K1, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=K2, shuffle=True, random_state=42)\n",
    "\n",
    "test_results =[]\n",
    "inner_errors = []\n",
    "\n",
    "# Outer Loop\n",
    "for i, (train_idx, test_idx) in enumerate(outer_cv.split(X_tensor)):\n",
    "    X_train, X_test = X_tensor[train_idx], X_tensor[test_idx]\n",
    "    y_train, y_test = y_tensor[train_idx], y_tensor[test_idx]\n",
    "\n",
    "    best_h = None\n",
    "    #best_lambda = None\n",
    "    best_model = None\n",
    "    lowest_error = float('inf')\n",
    "\n",
    "    # Inner Loop\n",
    "    for h in range(1,20): # onyl try up to 20 hidden neurons\n",
    "        inner_errors_for_fold = []\n",
    "\n",
    "        for train_idx_in, val_idx in inner_cv.split(X_train):\n",
    "            X_train_inner, X_val = X_train[train_idx_in], X_train[val_idx]\n",
    "            y_train_inner, y_val = y_train[train_idx_in], y_train[val_idx]\n",
    "\n",
    "            # Define model based on h  \n",
    "            model = ANN(X_train_inner.shape[1], h)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            # Training phase\n",
    "            for epoch in range(100):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_train_inner)\n",
    "                loss = criterion(outputs, y_train_inner)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "\n",
    "            # Evaluation phase\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val)\n",
    "                #val_loss = criterion(val_outputs, y_val)\n",
    "                val_error = mean_squared_error(y_val.numpy(), val_outputs.numpy())\n",
    "                inner_errors_for_fold.append(val_error)\n",
    "            \n",
    "        \n",
    "        # Find best h for this fold\n",
    "        avg_inner_error = np.mean(inner_errors_for_fold)\n",
    "        if avg_inner_error < lowest_error: \n",
    "            lowest_error = avg_inner_error\n",
    "            best_h = h\n",
    "            \n",
    "                \n",
    "\n",
    "    # Evaluate the best model based on best h\n",
    "    best_model = ANN(X_train.shape[1], best_h)\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(50):\n",
    "        best_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = best_model(X_test)\n",
    "        test_error = mean_squared_error(y_test.numpy(), test_outputs.numpy())\n",
    "\n",
    "    test_results.append((i+1, best_h, test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_classification_model(y_train,y_test):\n",
    "    classifiers,counts = np.unique(y_train,return_counts=True)\n",
    "    classifier = classifiers[np.where(counts==counts.max())][0]\n",
    "    print(classifier)\n",
    "    return np.count_nonzero(classifier==y_test)/len(y_test)\n",
    "baseline_classification_model(y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Naive Classifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#def naive_classifier(X, y):\n",
    "#def naive_classifier(X_train,y_train,X_test,y_test):\n",
    "np.random.seed(2450)\n",
    "y = y.squeeze()\n",
    "    #0\n",
    "    # Naive Bayes classifier parameters\n",
    "alpha = 1.0  # pseudo-count, additive parameter (Laplace correction if 1.0 or Lidtstone smoothing otherwise)\n",
    "fit_prior = True  # uniform prior (change to True to estimate prior from data)\n",
    "    # K-fold crossvalidation\n",
    "K = 10\n",
    "CV = model_selection.KFold(n_splits=K, shuffle=True)\n",
    "\n",
    "    #X = X[:, 0:4]  # using all 4 letters. Not needed here, because we already have defined that on top?\n",
    "    #X = OneHotEncoder().fit_transform(X=X)\n",
    "\n",
    "errors = np.zeros(K)\n",
    "train_errors = np.zeros(K)\n",
    "test_errors = np.zeros(K)\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X):\n",
    "    print('Crossvalidation fold: {0}/{1}'.format(k+1,K))\n",
    "\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    nb_classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    y_est_prob = nb_classifier.predict_proba(X_test)\n",
    "    y_est = np.argmax(y_est_prob, 1)\n",
    "\n",
    "    # Predictions for test and train sets\n",
    "    y_test_est = nb_classifier.predict(X_test)\n",
    "    y_train_est = nb_classifier.predict(X_train)\n",
    "\n",
    "    # Calculate error rates\n",
    "    test_errors[k] = np.mean(y_test_est != y_test)\n",
    "    train_errors[k] = np.mean(y_train_est != y_train)\n",
    "\n",
    "    errors[k] = np.sum(y_est != y_test, dtype=float) / y_test.shape[0]\n",
    "    k += 1\n",
    "\n",
    "    # Plot the classification error rate\n",
    "    #print(\"Error rate: {0}%\".format(100 * np.mean(errors))) \n",
    "    #print(\"Coefficient Norm:\", coefficient_norm[k])\n",
    "    print(\"Average Train Error Rate: {:.2f}%\".format(100 * np.mean(train_errors)))\n",
    "    print(\"Average Test Error Rate: {:.2f}%\".format(100 * np.mean(test_errors)))\n",
    "    \n",
    "#naive_classifier(X_train,y_train,X_test,y_test)\n",
    "#naive_classifier(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "data = pd.read_csv(cwd + '/usa_wood_data_formatted.csv')\n",
    "#data = pd.read_csv('C:/Users/ongji/OneDrive/Documents/DTU/02450 Introduction to Machine Learning and Data Mining/02450-Project-1/usa_wood_data_formatted.csv')\n",
    "# print(data.head())  \n",
    "\n",
    "# Clean data by removing columns with a significant amount of missing values\n",
    "removed_columns = ['side_hardness','tension_perpendicular_to_grain','impact_bending']\n",
    "data_cleaned = data.drop(columns=removed_columns)\n",
    "data_cleaned = data_cleaned.dropna()\n",
    "# print(data_cleaned.isnull().sum()) # 0 means data is clean\n",
    "\n",
    "\n",
    "# Extract attribute names (1st row, column 0 to 13)\n",
    "attributeNames = data_cleaned.columns[:13]\n",
    "for i in range(len(attributeNames)):\n",
    "    print(i, attributeNames[i])\n",
    "\n",
    "# Extract class names to python list, then encode with integers (dict)\n",
    "classLabels = data_cleaned['classification'].values\n",
    "classNames = sorted(set(classLabels))\n",
    "classDict = {className: index for index, className in enumerate(classNames)}\n",
    "y = np.array([classDict[label] for label in classLabels])\n",
    "print(y)\n",
    "\n",
    "# Extract class names to python list, then encode with integers (dict)\n",
    "classLabels = data_cleaned['moisture'].values\n",
    "classNames = sorted(set(classLabels))\n",
    "classDict = {className: index for index, className in enumerate(classNames)}\n",
    "y = np.array([classDict[label] for label in classLabels])\n",
    "print(y)\n",
    "\n",
    "\n",
    "# Preallocate memory, then extract excel data to matrix X\n",
    "X = data_cleaned.iloc[:,0:13].values\n",
    "\n",
    "# Compute values of N, M and C.\n",
    "N = len(y)\n",
    "M = len(attributeNames)\n",
    "C = len(classNames)\n",
    "\n",
    "# Filter out numerical values\n",
    "X_num = data_cleaned.iloc[:,6:].values\n",
    "\n",
    "# Scale and substract mean from data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_num)\n",
    "Y = X_scaled - np.ones((N, 1)) * X_scaled.mean(axis=0)\n",
    "\n",
    "# PCA by computing SVD of Y\n",
    "U, S, Vh = svd(Y, full_matrices=False)\n",
    "\n",
    "# Compute variance explained by principal components\n",
    "rho = (S * S) / (S * S).sum()\n",
    "\n",
    "# print(\"Singular values:\", S)\n",
    "# print(\"Variance explained:\", rho)\n",
    "# print(\"Cumulative variance explained:\", np.cumsum(rho))\n",
    "\n",
    "# Convert arra\n",
    "w = X_scaled[:,1:]\n",
    "X_tensor = torch.FloatTensor(w)\n",
    "#y_tensor = torch.FloatTensor(X_scaled[:,6])\n",
    "y_tensor = torch.FloatTensor(X_scaled[:,1]).unsqueeze(1)\n",
    "print(\"Xtensor:\", X_tensor)\n",
    "print(\"ytensor:\",y_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
